{"cells":[{"cell_type":"markdown","metadata":{"id":"qu7nrq41QJiL"},"source":["### Import"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35578,"status":"ok","timestamp":1739611857481,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"y3xv7hYGTi4A","outputId":"626f2895-d629-4bba-ca61-c3e0f596cdc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting category_encoders\n","  Downloading category_encoders-2.8.0-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.26.4)\n","Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n","Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.5.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n","Downloading category_encoders-2.8.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: category_encoders\n","Successfully installed category_encoders-2.8.0\n","Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}],"source":["!pip install category_encoders\n","!pip install catboost"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15294,"status":"ok","timestamp":1739611874728,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"4ey8-1l_QJiO","outputId":"7845580d-9eda-466b-fde5-e14e0e4c5fa0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]}],"source":["import random\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import os\n","import json\n","import re\n","\n","from sklearn.linear_model import (\n","    LogisticRegression\n",")\n","\n","from sklearn.ensemble import (\n","    ExtraTreesClassifier,\n","    RandomForestClassifier\n",")\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    confusion_matrix,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n","    roc_auc_score\n",")\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import StratifiedKFold\n","\n","import category_encoders as ce\n","\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","from xgboost import XGBClassifier\n","\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"markdown","metadata":{"id":"I5nWl4UXQJiP"},"source":["### Data Load"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20018,"status":"ok","timestamp":1739612006528,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"Uoab0X_lQRua","outputId":"149926d8-fd99-4af9-90e6-fcabc39fe3b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1739612008740,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"8x1vnRQ8Qwkg","outputId":"c113d01b-17bc-4637-b8c3-dfb7acfb5ba3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content/drive/MyDrive/Aimers_6th/Data\n"]}],"source":["import os\n","\n","# 현재 작업 디렉터리 출력\n","print(os.getcwd())\n","\n","data_path = os.path.join(os.getcwd(), 'drive', 'MyDrive', 'Aimers_6th', 'Data')\n","print(data_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":245,"status":"ok","timestamp":1739612010365,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"cljkaVMJCF58"},"outputs":[],"source":["config = {\n","    'root': data_path\n","    , 'train_path': f'{data_path}/train.csv'\n","    , 'test_path': f'{data_path}/test.csv'\n","    , 'submit_path': f'{data_path}/sample_submission.csv'\n","    , 'seed_list': [42]\n","    , 'k_fold': 5\n","    , 'model': 'lgb'       # cbt, logistic, et, rf, lgb, xgb\n","    , 'encoding': None # None(lgb, xgb, cbt), target, one-hot, ordinal, catboost\n","\n","}"]},{"cell_type":"markdown","metadata":{"id":"SKK9T6wdC7tq"},"source":["### HyperParameter"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1739612014197,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"FM3otssjC-ur"},"outputs":[],"source":["# Our best parameters\n","params = {\n","    'logistic': {\n","        'n_jobs': 1,\n","        'random_state': config['seed_list'][0],\n","        'max_iter': 300,\n","        'penalty': 'l2'\n","    },\n","\n","    'et': {\n","        'n_jobs': 1,\n","        'random_state': config['seed_list'][0],\n","        'n_estimators': 300,\n","        'max_depth': 6,\n","        'min_samples_leaf': 2,\n","        'max_samples': 0.5\n","\n","    },\n","\n","    'rf': {\n","        'n_jobs': 1,\n","        'random_state': config['seed_list'][0],\n","        'n_estimators': 300,\n","        'max_depth': 6,\n","        'min_samples_leaf': 2,\n","        # 'max_samples': 0.5,\n","        'bootstrap': False,\n","\n","    },\n","\n","    'lgb': {\n","        'random_state': config['seed_list'][0],\n","        'objective': 'binary',\n","        'n_jobs': 1,\n","        'verbosity': -1,\n","        'early_stopping_rounds': 10,\n","        'n_estimators': 300,\n","        'learning_rate': 0.1,\n","        'max_depth': 6,\n","        'reg_lambda': 1,\n","        'subsample': 0.5,\n","        'deterministic': True,\n","    },\n","\n","    'xgb': {\n","        'random_state': config['seed_list'][0],\n","        'objective': 'binary:logistic',\n","        'eval_metric': 'auc',\n","        'n_jobs': 1,\n","        'early_stopping_rounds': 10,\n","        'learning_rate': 0.1,\n","        'n_estimators': 300,\n","        'max_depth': 6,\n","        'reg_lambda': 1,\n","        'subsample': 0.5,\n","        'enable_categorical': True,\n","        'tree_method': 'hist'\n","\n","    },\n","\n","    'cbt': {\n","        'random_seed': config['seed_list'][0],\n","        'objective': 'Logloss',\n","        'eval_metric': 'AUC',\n","        'auto_class_weights': 'Balanced',\n","        'verbose': 100,\n","        'early_stopping_rounds': 10,\n","        'learning_rate': 0.1,\n","        'n_estimators': 300,\n","        'max_depth': 6,\n","        'l2_leaf_reg': 1,\n","        'min_data_in_leaf': 2,\n","        'subsample': 0.5,\n","        'task_type': 'CPU',\n","        'allow_writing_files': False\n","    }\n","}"]},{"cell_type":"markdown","metadata":{"id":"gwV9ZY6PDFoi"},"source":["### Function"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1739614135946,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"0D-Kj4MNDKIC"},"outputs":[],"source":["def set_seed(seed: int):\n","    # Set the seed for reproducibility.\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","\n","def read_data(config):\n","    # Load training, testing, and submission CSV files\n","    df_train = pd.read_csv(config['train_path']).drop(columns=['ID'])  # train data\n","    df_test = pd.read_csv(config['test_path']).drop(columns=['ID'])    # test data\n","    df_sub = pd.read_csv(config['submit_path'])\n","\n","    print(f'train data 수: {df_train.shape[0]}')\n","    print(f'test data 수: {df_test.shape[0]}')\n","    print(f'submission data 수: {df_sub.shape[0]}')\n","    return df_train, df_test, df_sub\n","\n","def df_ivf_di_split(df, is_train=False):\n","\n","    data_type = 'train' if is_train else 'test'\n","\n","    print(f'data 수: {df.shape[0]}')\n","\n","    df_ivf = df[df['시술 유형'] == 'IVF'].drop(columns=['시술 유형'])\n","    df_di = df[df['시술 유형'] == 'DI'].drop(columns=['시술 유형'])\n","\n","    print(f'시술유형 IVF {data_type} data 수: {df_ivf.shape[0]}')\n","    print(f'시술유형 DI {data_type} data 수: {df_di.shape[0]}')\n","\n","    return df_ivf, df_di\n","\n","\n","def get_clf_eval(y_test, y_proba=None, fold_no=None):\n","    # Calculate and print evaluation metrics and confusion matrix,\n","    # accuracy, precision, recall, f1 and roc_auc score.\n","    # Optionally includes fold number in the output.\n","\n","    # 임계값 0.5 기준 예측값 생성\n","    y_pred = (y_proba >= 0.5).astype(int)\n","\n","    y_test = y_test.values\n","\n","    confusion = confusion_matrix(y_test, y_pred)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)  # ROC-AUC는 확률값 그대로 사용\n","\n","    fold_info = f'Fold #{fold_no}' if fold_no is not None else ''\n","    print(f'{fold_info} ACC: {accuracy:.4f}, PRE: {precision:.4f}, REC: {recall:.4f}, F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}')\n","    return roc_auc\n","\n","\n","# Categorical variable encoding method.\n","def category_encoding(df_train, df_test, base_num_features, base_cat_features, config, is_ivf=False):\n","    target = '임신 성공 여부'\n","\n","    data_name = 'IVF' if is_ivf else 'DI'\n","\n","    new_df_train = pd.concat([df_train[base_num_features].copy(), df_train[target]], axis = 1)\n","    new_df_test = df_test[base_num_features].copy()\n","\n","    print(f'{data_name} category 변수 인코딩: {config[\"encoding\"]}')\n","\n","    if config['encoding'] == 'target':\n","        encoder = ce.TargetEncoder(cols=base_cat_features)\n","\n","    elif config['encoding'] == 'ordinal':\n","        encoder = ce.OrdinalEncoder(cols=base_cat_features)\n","\n","    elif config['encoding'] == 'catboost':\n","        encoder = ce.CatBoostEncoder(cols=base_cat_features)#, random_state=config['seed'])\n","\n","    if config['encoding'] in ['target', 'ordinal', 'catboost']:\n","        encoder.fit(df_train[base_cat_features], df_train[target])\n","        new_df_train[base_cat_features] = encoder.transform(df_train[base_cat_features])\n","        new_df_test[base_cat_features] = encoder.transform(df_test[base_cat_features])\n","\n","    elif config['encoding'] == 'one-hot':\n","        encoder = ce.OneHotEncoder(cols=base_cat_features, use_cat_names = True)\n","        encoder.fit(df_train[base_cat_features], df_train[target])\n","        result_tr = encoder.transform(df_train[base_cat_features])\n","        result_te = encoder.transform(df_test[base_cat_features])\n","\n","        result_tr.columns = result_tr.columns.str.replace(r'[^ㄱ-ㅎ가-힣A-Za-z0-9_]', '_', regex=True)\n","        result_te.columns = result_te.columns.str.replace(r'[^ㄱ-ㅎ가-힣A-Za-z0-9_]', '_', regex=True)\n","\n","        new_df_train = pd.concat([new_df_train, result_tr], axis = 1)\n","        new_df_test = pd.concat([new_df_test, result_te], axis = 1)\n","\n","    elif config['encoding'] is None:\n","        new_df_train = pd.concat([new_df_train, df_train[base_cat_features]], axis = 1)\n","        new_df_test = pd.concat([new_df_test, df_test[base_cat_features]], axis = 1)\n","\n","    _, base_cat_features, base_features = make_feature_lists(new_df_train, is_ivf=is_ivf)\n","\n","    return new_df_train, new_df_test, base_cat_features, base_features\n","\n","def feature_engineering(df_input, tr_removed_column_list, is_train=False, is_ivf=False):\n","    df = df_input.copy()\n","\n","    data_name = 'IVF' if is_ivf else 'DI'\n","    data_type = 'train' if is_train else 'test'\n","\n","    print(f'중복 제거 전 {data_type} {data_name} data 수: {df.shape[0]}')\n","\n","    if is_train:\n","        removed_column_list = []\n","    else:\n","        removed_column_list = tr_removed_column_list\n","\n","    # drop_duplicates\n","    df = df.drop_duplicates(keep='first')\n","\n","\n","    print(f'중복 제거 후 {data_type} {data_name} data 수: {df.shape[0]}')\n","\n","    df_column_list = df.columns\n","\n","    print(f'column 제거 전 {data_type} {data_name} data shape: {df.shape}')\n","\n","\t\t# 결측치 채우기\n","    df['착상 전 유전 검사 사용 여부'] = df['착상 전 유전 검사 사용 여부'].fillna(0)\n","    df['PGD 시술 여부'] = df['PGD 시술 여부'].fillna(0)\n","    df['PGS 시술 여부'] = df['PGS 시술 여부'].fillna(0)\n","    df['난자 채취 경과일'] = df['난자 채취 경과일'].fillna(-1) # 값이 1개 뿐이라 제거 하지 않으려면 채워야함\n","\n","    if is_train:\n","        # train data에서 전부 결측치거나 값 1개만 갖는 column 제거\n","\n","        for feat in df_column_list:\n","            if df[feat].nunique() <= 1:\n","                df = df.drop(columns=[feat])\n","                removed_column_list.append(feat)\n","\n","\n","    # test data에서는 train data에서 제거된 column 제거\n","    else:\n","\n","        df = df.drop(columns=removed_column_list)\n","\n","    print(f'column 제거 후 {data_type} {data_name} data shape: {df.shape}')\n","\n","    # 횟수 관련 column\n","    count_columns = [column for column in df.columns if '횟수' in column]\n","\n","    for col in count_columns:\n","        df[col] = df[col].str.replace('회', '').str.replace('이상', '').str.replace(' ', '').astype(int)\n","\n","\n","    # # 불임 원인 multi hot -> 하나의 feature\n","\n","    # infertility_columns = [column for column in df.columns if '불임 원인' in column]\n","\n","    # # dot()을 사용하여 1이 있는 컬럼을 결합 (각 feature 사이에 공백 추가)\n","    # df['불임 원인 종합'] = df[infertility_columns].dot(pd.Index([f'{col.replace(\"불임 원인\", \"\").replace(\" \", \"\")} ' for col in infertility_columns])).str.strip()\n","\n","    # # 한글만 남기고 나머지 문자 제거 (정규 표현식 사용)\n","    # df['불임 원인 종합'] = df['불임 원인 종합'].apply(lambda x: re.sub(r'[^가-힣 ]', '', x))\n","\n","    # # split()과 join()을 사용하여 공백을 ','로 변경\n","    # df['불임 원인 종합'] = df['불임 원인 종합'].apply(lambda x: ', '.join(x.split()))\n","\n","    # df = df.drop(columns=infertility_columns)\n","\n","    return df, removed_column_list\n","\n","def make_feature_lists(df, is_ivf=False):\n","    base_features = []     # all features except target variable.\n","    base_num_features = [] # numerical features\n","    base_cat_features = [] # categorical features\n","\n","    data_name = 'IVF' if is_ivf else 'DI'\n","\n","    for feat in df.columns:\n","        # skip the target\n","        if feat == '임신 성공 여부':\n","            continue\n","\n","        base_features.append(feat)\n","\n","        if df[feat].dtype in ['object', 'category']:\n","            base_cat_features.append(feat)\n","        else:\n","            base_num_features.append(feat)\n","\n","    # infertility_columns = [column for column in df.columns if '불임 원인' in column]\n","\n","    # 공통 제거 변수\n","    removal_features = {\n","            'ID', '불임 원인 - 여성 요인', '불임 원인 - 정자 면역학적 요인', '불임 원인 - 자궁경부 문제', '시술 유형',\n","\n","\n","    }\n","\n","    # ivf 제거 변수\n","    removal_features_ivf = {\"미세주입된 난자 수\", \"혼합된 난자 수\",\"파트너 정자와 혼합된 난자 수\"\n","    }\n","\n","    # di 제거 변수\n","    removal_features_di = {\n","    }\n","\n","    if is_ivf:\n","        removal_features = removal_features.union(removal_features_ivf)\n","    else:\n","        removal_features = removal_features.union(removal_features_di)\n","\n","    # removal_features.update(infertility_columns)\n","\n","    # remove the specified features\n","    base_num_features = [i for i in base_num_features if i not in removal_features]\n","    base_cat_features = [i for i in base_cat_features if i not in removal_features]\n","    base_features = [i for i in base_features if i not in removal_features]\n","\n","    print(f'{data_name} numeric feature 수: {len(base_num_features)}')\n","    print(f'{data_name} category feature 수: {len(base_cat_features)}')\n","    print(f'{data_name} 전체 feature 수: {len(base_features)}')\n","\n","    return base_num_features, base_cat_features, base_features\n","\n","def filling_missing_values(df_input, base_cat_features, base_num_features, config):\n","    df = df_input.copy()\n","\n","    # Fill missing values for categorical features with 'Unknown' and ensure their data type is string.\n","    for base_cat_feat in base_cat_features:\n","        df[base_cat_feat] = df[base_cat_feat].astype(str)\n","        df[base_cat_feat] = df[base_cat_feat].fillna('알 수 없음')\n","\n","        if config['encoding'] is None:\n","            df[base_cat_feat] = df[base_cat_feat].astype('category')\n","\n","\n","    # Fill missing values for numerical features with -1.\n","    for base_num_feat in base_num_features:\n","        df[base_num_feat] = df[base_num_feat].fillna(-1)\n","\n","    return df\n","\n","\n","def model_kfold(df, config, params, base_features, base_cat_features, is_ivf=False):\n","    target = '임신 성공 여부'\n","\n","    data_name = 'IVF' if is_ivf else 'DI'\n","\n","    skf = StratifiedKFold(n_splits=config['k_fold'], shuffle=True, random_state=config['seed'])\n","    models = []       # trained models\n","    roc_auc_scores = []  # roc_auc_scores for validation sets\n","\n","    model_params = params[config['model']]\n","\n","    for k_fold, (train_idx, valid_idx) in enumerate(skf.split(df[base_features], df[target])):\n","        print(f'Fold #{k_fold + 1}')\n","        X_train, y_train = df[base_features].iloc[train_idx], df[target].iloc[train_idx]\n","        X_valid, y_valid = df[base_features].iloc[valid_idx], df[target].iloc[valid_idx]\n","\n","        if config['model'] == 'logistic':\n","            model = LogisticRegression(**model_params)\n","\n","        elif config['model'] == 'et':\n","            model = ExtraTreesClassifier(**model_params)\n","\n","        elif config['model'] == 'rf':\n","            model = RandomForestClassifier(**model_params)\n","\n","        if config['model'] in ['logistic', 'et', 'rf']:\n","            model.fit(X_train, y_train)\n","\n","        elif config['model'] == 'lgb':\n","            model = LGBMClassifier(**model_params)\n","\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid, y_valid)],\n","                eval_metric='auc',\n","                categorical_feature=base_cat_features, # specify categorical features\n","            )\n","\n","        elif config['model'] == 'xgb':\n","            model = XGBClassifier(**model_params)\n","\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid, y_valid)],\n","                verbose = 100\n","            )\n","\n","        elif config['model'] == 'cbt':\n","            model = CatBoostClassifier(**model_params)\n","\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid, y_valid)],\n","                cat_features=base_cat_features, # specify categorical features\n","            )\n","\n","        # save the trained model\n","        models.append(model)\n","\n","        # evaluate the model\n","        # --- train-set\n","        print(f'[{data_name} Train] ', end='')\n","        y_prob = model.predict_proba(X_train)[:, 1]\n","        _ = get_clf_eval(y_train, y_prob, k_fold + 1)\n","\n","        # --- valid-set\n","        print(f'[{data_name} Valid] ', end='')\n","        y_prob = model.predict_proba(X_valid)[:, 1]\n","        roc_auc_score = get_clf_eval(y_valid, y_prob, k_fold + 1)\n","\n","        roc_auc_scores.append(roc_auc_score)\n","\n","    avg_roc_auc = np.mean(roc_auc_scores)\n","    var_roc_auc = np.var(roc_auc_scores)\n","\n","    print(f'Avg. roc-auc of validset {data_name}: {avg_roc_auc}')\n","    print(f'Var. roc-auc of validset {data_name}: {var_roc_auc}')\n","\n","    return models, avg_roc_auc, var_roc_auc\n","\n","def kfold_submission(df_test_ivf, df_test_di, df_sub, models_ivf, models_di, config):\n","    feat_importance_path = f\"{config['root']}/FeatureImportance\"\n","    submission_path = f\"{config['root']}/Submission\"\n","    json_path = f\"{config['root']}/Json\"\n","\n","    if not os.path.exists(feat_importance_path):\n","        os.makedirs(feat_importance_path)\n","\n","    if not os.path.exists(submission_path):\n","        os.makedirs(submission_path)\n","\n","    if not os.path.exists(json_path):\n","        os.makedirs(json_path)\n","\n","    # get current date and time\n","    now = datetime.now()\n","\n","    # record the year, month, day, hour, and minute for naming files.\n","    year = now.year\n","    month = now.month\n","    day = now.day\n","    hour = now.hour\n","    minute = now.minute\n","\n","    # file format\n","    submission_time = f\"{year:04d}{month:02d}{day:02d}_{hour:02d}{minute:02d}\"[2:]\n","    target = 'probability'\n","\n","    # apply feature engineering\n","    base_num_features_ivf, base_cat_features_ivf, base_features_ivf = make_feature_lists(df_test_ivf)\n","    df_test_ivf = filling_missing_values(df_test_ivf, base_cat_features_ivf, base_num_features_ivf, config)\n","    X_test_ivf = df_test_ivf[base_features_ivf]\n","\n","    base_num_features_di, base_cat_features_di, base_features_di = make_feature_lists(df_test_di)\n","    df_test_di = filling_missing_values(df_test_di, base_cat_features_di, base_num_features_di, config)\n","    X_test_di = df_test_di[base_features_di]\n","\n","    # dataframe for feature importances\n","    base_features = list(set(base_features_ivf).union(set(base_features_di)))\n","\n","    df_feature_importance_all_ivf = pd.DataFrame({'features': base_features_ivf})\n","    df_feature_importance_all_di = pd.DataFrame({'features': base_features_di})\n","\n","    y_probs_ivf = 0\n","    y_probs_di = 0\n","\n","    for i, model_ivf in enumerate(models_ivf):\n","        y_probs_ivf += model_ivf.predict_proba(X_test_ivf)[:, 1] / len(models_ivf)\n","\n","        # save feature importance of current model\n","        if config['model'] in ['logistic']:\n","            df_feature_importance_all_ivf[f'model_{i}'] = model_ivf.coef_.squeeze()\n","\n","        elif config['model'] in ['et', 'rf', 'lgb', 'xgb']:\n","            df_feature_importance_all_ivf[f'model_{i}'] = model_ivf.feature_importances_\n","\n","        elif config['model'] == 'cbt':\n","            df_feature_importance_all_ivf[f'model_{i}'] = model_ivf.get_feature_importance()\n","\n","    for i, model_di in enumerate(models_di):\n","        y_probs_di += model_di.predict_proba(X_test_di)[:, 1] / len(models_di)\n","\n","        # save feature importance of current model\n","        if config['model'] in ['logistic']:\n","            df_feature_importance_all_di[f'model_{i}'] = model_di.coef_.squeeze()\n","\n","        elif config['model'] in ['et', 'rf', 'lgb', 'xgb']:\n","            df_feature_importance_all_di[f'model_{i}'] = model_di.feature_importances_\n","\n","        elif config['model'] == 'cbt':\n","            df_feature_importance_all_di[f'model_{i}'] = model_di.get_feature_importance()\n","\n","    df_sub.loc[df_test_ivf.index, target] = y_probs_ivf\n","    df_sub.loc[df_test_di.index, target] = y_probs_di\n","\n","    # save submission file as CSV\n","    df_sub.to_csv(f\"{submission_path}/{submission_time}_{config['model']}_{config['encoding']}_submission.csv\", index=False)\n","\n","    # compute avarege, rank\n","    df_feature_importance_all_ivf['average'] = df_feature_importance_all_ivf.iloc[:, 1:].mean(axis=1).values\n","    df_feature_importance_all_ivf['rank'] = df_feature_importance_all_ivf['average'].rank(ascending=False)\n","\n","    df_feature_importance_all_di['average'] = df_feature_importance_all_di.iloc[:, 1:].mean(axis=1).values\n","    df_feature_importance_all_di['rank'] = df_feature_importance_all_di['average'].rank(ascending=False)\n","\n","    # save the feature importance as CSV\n","    df_feature_importance_all_ivf.to_csv(f'{feat_importance_path}/feat_import_{submission_time}_{config[\"model\"]}_IVF_{config[\"encoding\"]}.csv', index=False)\n","    df_feature_importance_all_di.to_csv(f'{feat_importance_path}/feat_import_{submission_time}_{config[\"model\"]}_DI_{config[\"encoding\"]}.csv', index=False)\n","\n","    # save parameters as JSON\n","    json_data = json.dumps(config, indent=4)\n","\n","    with open(f'{json_path}/{submission_time}_{config[\"model\"]}_{config[\"encoding\"]}.json', 'w') as file:\n","        file.write(json_data)"]},{"cell_type":"markdown","metadata":{"id":"3d4zUcXXQJiQ"},"source":["### Data Pre-processing"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":288,"status":"ok","timestamp":1739614140361,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"7wSJYvkYESQv"},"outputs":[],"source":["def main(config, params):\n","    models_ivf = []\n","    models_di = []\n","\n","    for seed in config['seed_list']:\n","        config['seed'] = seed\n","        params['random_seed'] = seed\n","\n","        # set seed\n","        set_seed(config['seed'])\n","\n","        # read data set\n","        df_train, df_test, df_sub = read_data(config)\n","\n","        # ivf di split\n","        df_train_ivf, df_train_di = df_ivf_di_split(df_train, is_train = True)\n","        df_test_ivf, df_test_di = df_ivf_di_split(df_test, is_train = False)\n","\n","        # feature engineering (train,test)\n","        df_train_ivf, removed_column_list_ivf = feature_engineering(df_train_ivf, None, is_train=True, is_ivf=True)\n","        df_test_ivf, _ = feature_engineering(df_test_ivf, removed_column_list_ivf, is_train=False, is_ivf=True)\n","\n","        df_train_di, removed_column_list_di = feature_engineering(df_train_di, None, is_train=True, is_ivf=False)\n","        df_test_di, _ = feature_engineering(df_test_di, removed_column_list_di, is_train=False, is_ivf=False)\n","\n","        # feature list 생성 (train)\n","        base_num_features_ivf, base_cat_features_ivf, base_features_ivf = make_feature_lists(df_train_ivf, is_ivf=True)\n","        base_num_features_di, base_cat_features_di, base_features_di = make_feature_lists(df_train_di, is_ivf=False)\n","\n","        # 결측치 처리\n","        df_train_ivf = filling_missing_values(df_train_ivf, base_cat_features_ivf, base_num_features_ivf, config)\n","        df_train_di = filling_missing_values(df_train_di, base_cat_features_di, base_num_features_di, config)\n","\n","        # encoding\n","        df_train_ivf, df_test_ivf, base_cat_features_ivf, base_features_ivf = category_encoding(df_train_ivf, df_test_ivf, base_num_features_ivf, base_cat_features_ivf, config, is_ivf=True)\n","        df_train_di, df_test_di, base_cat_features_di, base_features_di = category_encoding(df_train_di, df_test_di, base_num_features_di, base_cat_features_di, config, is_ivf=False)\n","\n","        # check model performance\n","        model_ivf, avg_roc_auc_ivf, var_roc_auc_ivf = model_kfold(df_train_ivf, config, params, base_features_ivf, base_cat_features_ivf, is_ivf=True)\n","        model_di, avg_roc_auc_di, var_roc_auc_di = model_kfold(df_train_di, config, params, base_features_di, base_cat_features_di, is_ivf=False)\n","\n","        config['avg_roc_auc_ivf'] = avg_roc_auc_ivf\n","        config['var_roc_auc_ivf'] = var_roc_auc_ivf\n","\n","        config['avg_roc_auc_di'] = avg_roc_auc_di\n","        config['var_roc_auc_di'] = var_roc_auc_di\n","\n","        models_ivf.extend(model_ivf)\n","        models_di.extend(model_di)\n","\n","    config['model_param'] = params[config['model']]\n","\n","    # submission\n","    kfold_submission(df_test_ivf, df_test_di, df_sub, models_ivf, models_di, config)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":55414,"status":"ok","timestamp":1739614197986,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"NaZgJXIyEVDj","outputId":"c25fbda1-56b5-4d87-966c-b9bd1d9f2514"},"outputs":[{"output_type":"stream","name":"stdout","text":["train data 수: 256351\n","test data 수: 90067\n","submission data 수: 90067\n","data 수: 256351\n","시술유형 IVF train data 수: 250060\n","시술유형 DI train data 수: 6291\n","data 수: 90067\n","시술유형 IVF test data 수: 87891\n","시술유형 DI test data 수: 2176\n","중복 제거 전 train IVF data 수: 250060\n","중복 제거 후 train IVF data 수: 250060\n","column 제거 전 train IVF data shape: (250060, 67)\n","column 제거 후 train IVF data shape: (250060, 66)\n","중복 제거 전 test IVF data 수: 87891\n","중복 제거 후 test IVF data 수: 87891\n","column 제거 전 test IVF data shape: (87891, 66)\n","column 제거 후 test IVF data shape: (87891, 65)\n","중복 제거 전 train DI data 수: 6291\n","중복 제거 후 train DI data 수: 6291\n","column 제거 전 train DI data shape: (6291, 67)\n","column 제거 후 train DI data shape: (6291, 30)\n","중복 제거 전 test DI data 수: 2176\n","중복 제거 후 test DI data 수: 2176\n","column 제거 전 test DI data shape: (2176, 66)\n","column 제거 후 test DI data shape: (2176, 29)\n","IVF numeric feature 수: 51\n","IVF category feature 수: 9\n","IVF 전체 feature 수: 60\n","DI numeric feature 수: 25\n","DI category feature 수: 4\n","DI 전체 feature 수: 29\n","IVF category 변수 인코딩: None\n","IVF numeric feature 수: 51\n","IVF category feature 수: 9\n","IVF 전체 feature 수: 60\n","DI category 변수 인코딩: None\n","DI numeric feature 수: 25\n","DI category feature 수: 4\n","DI 전체 feature 수: 29\n","Fold #1\n","[IVF Train] Fold #1 ACC: 0.7470, PRE: 0.5824, REC: 0.1156, F1: 0.1929, ROC-AUC: 0.7477\n","[IVF Valid] Fold #1 ACC: 0.7438, PRE: 0.5519, REC: 0.1097, F1: 0.1830, ROC-AUC: 0.7367\n","Fold #2\n","[IVF Train] Fold #2 ACC: 0.7476, PRE: 0.5842, REC: 0.1222, F1: 0.2022, ROC-AUC: 0.7470\n","[IVF Valid] Fold #2 ACC: 0.7416, PRE: 0.5292, REC: 0.1101, F1: 0.1822, ROC-AUC: 0.7390\n","Fold #3\n","[IVF Train] Fold #3 ACC: 0.7476, PRE: 0.5852, REC: 0.1207, F1: 0.2001, ROC-AUC: 0.7485\n","[IVF Valid] Fold #3 ACC: 0.7439, PRE: 0.5508, REC: 0.1135, F1: 0.1882, ROC-AUC: 0.7388\n","Fold #4\n","[IVF Train] Fold #4 ACC: 0.7464, PRE: 0.5881, REC: 0.1017, F1: 0.1733, ROC-AUC: 0.7446\n","[IVF Valid] Fold #4 ACC: 0.7418, PRE: 0.5378, REC: 0.0936, F1: 0.1595, ROC-AUC: 0.7371\n","Fold #5\n","[IVF Train] Fold #5 ACC: 0.7461, PRE: 0.5781, REC: 0.1087, F1: 0.1830, ROC-AUC: 0.7458\n","[IVF Valid] Fold #5 ACC: 0.7425, PRE: 0.5427, REC: 0.1010, F1: 0.1703, ROC-AUC: 0.7359\n","Avg. roc-auc of validset IVF: 0.7375055199534082\n","Var. roc-auc of validset IVF: 1.4237541203527478e-06\n","Fold #1\n","[DI Train] Fold #1 ACC: 0.8712, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.7445\n","[DI Valid] Fold #1 ACC: 0.8705, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.6461\n","Fold #2\n","[DI Train] Fold #2 ACC: 0.8711, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.7717\n","[DI Valid] Fold #2 ACC: 0.8712, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.7109\n","Fold #3\n","[DI Train] Fold #3 ACC: 0.8711, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.7832\n","[DI Valid] Fold #3 ACC: 0.8712, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.6621\n","Fold #4\n","[DI Train] Fold #4 ACC: 0.8720, PRE: 1.0000, REC: 0.0077, F1: 0.0153, ROC-AUC: 0.7895\n","[DI Valid] Fold #4 ACC: 0.8704, PRE: 0.0000, REC: 0.0000, F1: 0.0000, ROC-AUC: 0.6811\n","Fold #5\n","[DI Train] Fold #5 ACC: 0.8718, PRE: 1.0000, REC: 0.0062, F1: 0.0123, ROC-AUC: 0.8032\n","[DI Valid] Fold #5 ACC: 0.8728, PRE: 1.0000, REC: 0.0123, F1: 0.0244, ROC-AUC: 0.7023\n","Avg. roc-auc of validset DI: 0.6805018496957393\n","Var. roc-auc of validset DI: 0.0005834861764680666\n","DI numeric feature 수: 51\n","DI category feature 수: 9\n","DI 전체 feature 수: 60\n","DI numeric feature 수: 25\n","DI category feature 수: 4\n","DI 전체 feature 수: 29\n"]}],"source":["main(config, params)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"executionInfo":{"elapsed":293,"status":"error","timestamp":1739612213079,"user":{"displayName":"윤석진","userId":"14953914264612970479"},"user_tz":-540},"id":"VVQIHe7WBL_k","outputId":"e55d84b3-4dca-47e5-95e5-2ae4d972a509"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_train_ivf' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-0fca07c1a0b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf_train_ivf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train_ivf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{column}: {count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_train_ivf' is not defined"]}],"source":["unique_counts = {column: df_train_ivf[column].nunique() for column in df_train_ivf.columns}\n","for column, count in unique_counts.items():\n","    print(f\"{column}: {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1739280342717,"user":{"displayName":"유승태","userId":"01030021726785625805"},"user_tz":-540},"id":"HcsFotf1LWvB","outputId":"35e91cc6-95e8-4926-a9fb-cc0dcce56fd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["시술 시기 코드: 7\n","시술 당시 나이: 6\n","임신 시도 또는 마지막 임신 경과 연수: 19\n","특정 시술 유형: 5\n","배란 자극 여부: 2\n","배란 유도 유형: 1\n","단일 배아 이식 여부: 0\n","착상 전 유전 검사 사용 여부: 0\n","착상 전 유전 진단 사용 여부: 0\n","남성 주 불임 원인: 2\n","남성 부 불임 원인: 2\n","여성 주 불임 원인: 2\n","여성 부 불임 원인: 2\n","부부 주 불임 원인: 2\n","부부 부 불임 원인: 2\n","불명확 불임 원인: 2\n","불임 원인 - 난관 질환: 2\n","불임 원인 - 남성 요인: 2\n","불임 원인 - 배란 장애: 2\n","불임 원인 - 여성 요인: 1\n","불임 원인 - 자궁경부 문제: 1\n","불임 원인 - 자궁내막증: 2\n","불임 원인 - 정자 농도: 2\n","불임 원인 - 정자 면역학적 요인: 1\n","불임 원인 - 정자 운동성: 1\n","불임 원인 - 정자 형태: 2\n","배아 생성 주요 이유: 0\n","총 시술 횟수: 7\n","클리닉 내 총 시술 횟수: 7\n","IVF 시술 횟수: 7\n","DI 시술 횟수: 7\n","총 임신 횟수: 6\n","IVF 임신 횟수: 4\n","DI 임신 횟수: 6\n","총 출산 횟수: 4\n","IVF 출산 횟수: 3\n","DI 출산 횟수: 4\n","총 생성 배아 수: 0\n","미세주입된 난자 수: 0\n","미세주입에서 생성된 배아 수: 0\n","이식된 배아 수: 0\n","미세주입 배아 이식 수: 0\n","저장된 배아 수: 0\n","미세주입 후 저장된 배아 수: 0\n","해동된 배아 수: 0\n","해동 난자 수: 0\n","수집된 신선 난자 수: 0\n","저장된 신선 난자 수: 0\n","혼합된 난자 수: 0\n","파트너 정자와 혼합된 난자 수: 0\n","기증자 정자와 혼합된 난자 수: 0\n","난자 출처: 1\n","정자 출처: 1\n","난자 기증자 나이: 1\n","정자 기증자 나이: 7\n","동결 배아 사용 여부: 0\n","신선 배아 사용 여부: 0\n","기증 배아 사용 여부: 0\n","대리모 여부: 0\n","PGD 시술 여부: 0\n","PGS 시술 여부: 0\n","난자 채취 경과일: 0\n","난자 해동 경과일: 0\n","난자 혼합 경과일: 0\n","배아 이식 경과일: 0\n","배아 해동 경과일: 0\n","임신 성공 여부: 2\n"]}],"source":["unique_counts = {column: df_train_di[column].nunique() for column in df_train_di.columns}\n","for column, count in unique_counts.items():\n","    print(f\"{column}: {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVyKbeuBOZTY"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}